{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd682ae-d094-4bae-84fe-2e3f10aa9893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import col, SparkContext, udf\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589cccdf-5556-40ea-a306-484322f56f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Woodw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"C:\\Users\\Woodw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      5\u001b[0m conf \u001b[38;5;241m=\u001b[39m pyspark\u001b[38;5;241m.\u001b[39mSparkConf()\\\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.driver.host\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m127.0.0.1\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39msetAppName(appName)\\\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39msetMaster(master)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create Spark Context with the new configurations rather than relying on the default one\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# You need to create SQL Context to conduct some database operations like what we will see later.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m sqlContext \u001b[38;5;241m=\u001b[39m SQLContext(sc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\context.py:200\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    198\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\context.py:287\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\context.py:417\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1586\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1578\u001b[0m args_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m   1579\u001b[0m     [get_command_part(arg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[0;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1588\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "appName = \"Big Data ML\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "conf = pyspark.SparkConf()\\\n",
    "    .set('spark.driver.host','127.0.0.1')\\\n",
    "    .setAppName(appName)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "# Create Spark Context with the new configurations rather than relying on the default one\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# You need to create SQL Context to conduct some database operations like what we will see later.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# If you have SQL context, you create the session from the Spark Context\n",
    "# spark = sqlContext.sparkSession.builder.getOrCreate()\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f531d3-81fe-413d-b5ba-cae974854464",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqtt_raw = spark.read.csv(\"train70_reduced.csv\",header=True, inferSchema= True)\n",
    "mqtt_test_raw = spark.read.csv(\"test30_reduced.csv\",header=True, inferSchema= True)\n",
    "\n",
    "# Rename columns to better formats\n",
    "for column_name in mqtt_raw.columns:\n",
    "    new_column_name = column_name.replace(\".\", \"_\")\n",
    "    mqtt_raw = mqtt_raw.withColumnRenamed(column_name, new_column_name)\n",
    "\n",
    "for column_name in mqtt_test_raw.columns:\n",
    "    new_column_name = column_name.replace(\".\", \"_\")\n",
    "    mqtt_test_raw = mqtt_test_raw.withColumnRenamed(column_name, new_column_name)\n",
    "\n",
    "# Drop the few rows with NA values (only 190)\n",
    "mqtt_raw = mqtt_raw.na.drop()\n",
    "mqtt_test_raw = mqtt_test_raw.na.drop()\n",
    "\n",
    "# Dropping a couple useless columns\n",
    "mqtt_raw = mqtt_raw.drop(\"mqtt_protoname\")\n",
    "mqtt_test_raw = mqtt_test_raw.drop(\"mqtt_protoname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e8735c-24c2-480a-b3fc-206551fe1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqtt_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be9718-6e09-46cd-a3e9-05a6b3ede717",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names =  [\"tcp_flags\",\"tcp_time_delta\",\"tcp_len\",\n",
    "\"mqtt_conack_flags\",\"mqtt_conack_flags_reserved\",\"mqtt_conack_flags_sp\",\n",
    "\"mqtt_conack_val\",\"mqtt_conflag_cleansess\",\"mqtt_conflag_passwd\",\n",
    "\"mqtt_conflag_qos\",\"mqtt_conflag_reserved\",\"mqtt_conflag_retain\",\n",
    "\"mqtt_conflag_uname\",\"mqtt_conflag_willflag\",\n",
    "\"mqtt_conflags\",\"mqtt_dupflag\",\"mqtt_hdrflags\",\n",
    "\"mqtt_kalive\",\"mqtt_len\",\"mqtt_msg\",\"mqtt_msgid\",\"mqtt_msgtype\",\n",
    "\"mqtt_proto_len\",\"mqtt_qos\",\n",
    "\"mqtt_retain\",\"mqtt_sub_qos\",\"mqtt_suback_qos\",\"mqtt_ver\",\n",
    "\"mqtt_willmsg\",\"mqtt_willmsg_len\",\"mqtt_willtopic\",\"mqtt_willtopic_len\",\n",
    "\"target\"]\n",
    "\n",
    "nominal_cols = [\"mqtt_msgtype\", \"mqtt_conack_val\", \"tcp_flags\", \"mqtt_hdrflags\"]\n",
    "binary_cols = [\"mqtt_dupflag\"]\n",
    "continuous_cols = [\"tcp_time_delta\",\"tcp_len\",\n",
    "\"mqtt_conack_flags_reserved\",\"mqtt_conack_flags_sp\",\n",
    "\"mqtt_conflag_cleansess\",\"mqtt_conflag_passwd\",\n",
    "\"mqtt_conflag_qos\",\"mqtt_conflag_reserved\",\"mqtt_conflag_retain\",\n",
    "\"mqtt_conflag_uname\",\"mqtt_conflag_willflag\",\n",
    "\"mqtt_kalive\",\"mqtt_len\",\"mqtt_msg\",\"mqtt_msgid\",\n",
    "\"mqtt_proto_len\",\"mqtt_qos\",\n",
    "\"mqtt_retain\",\"mqtt_sub_qos\",\"mqtt_suback_qos\",\"mqtt_ver\",\n",
    "\"mqtt_willmsg\",\"mqtt_willmsg_len\",\"mqtt_willtopic\",\"mqtt_willtopic_len\",\n",
    "\"mqtt_conack_flags\", \"mqtt_conflags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ee95a4-ceff-4ec8-9a9d-d8b84cedf9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any columns that we want to just get rid of here\n",
    "# A large portion of these were dropped because they were a constant value\n",
    "columns_to_drop = [\"mqtt_msg\", \"mqtt_conack_flags\", \"mqtt_conflags\", \n",
    "\"mqtt_willtopic_len\", \"mqtt_willtopic\", \"mqtt_willmsg_len\", \"mqtt_willmsg\",\n",
    "\"mqtt_suback_qos\", \"mqtt_sub_qos\", \"mqtt_conflag_willflag\", \"mqtt_conflag_retain\",\n",
    "\"mqtt_conflag_reserved\", \"mqtt_conflag_qos\", \"mqtt_conack_flags_sp\", \n",
    "\"mqtt_conack_flags_reserved\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d960f5-59bd-4901-a8d5-a02777941b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data preprocessing pipeline\n",
    "'''\n",
    "class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def label_to_vector(self, name):\n",
    "        name = name.lower()\n",
    "        \n",
    "        if name == 'legitimate':\n",
    "            return 0.0\n",
    "        elif name == 'dos':\n",
    "            return 1.0\n",
    "        elif name == 'malformed':\n",
    "            return 2.0\n",
    "        elif name == 'slowite':\n",
    "            return 3.0\n",
    "        elif name == 'bruteforce':\n",
    "            return 4.0\n",
    "        elif name == 'flood':\n",
    "            return 5.0\n",
    "        else:\n",
    "            return -100.0\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        label_to_vector_udf = udf(self.label_to_vector, StringType())\n",
    "        output_df = dataset.withColumn('outcome', label_to_vector_udf(col('target'))).drop(\"target\")  \n",
    "        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "        return output_df\n",
    "\n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self, targetCols):\n",
    "        super().__init__()\n",
    "        self.target_cols = targetCols\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.target_cols:\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
    "\n",
    "        return output_df\n",
    "\n",
    "class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "def get_preprocess_pipeline():\n",
    "    # Stage where columns are casted as appropriate types\n",
    "    impute_columns = binary_cols+continuous_cols\n",
    "    for col_name in columns_to_drop:\n",
    "        if col_name in impute_columns:\n",
    "            impute_columns.remove(col_name)\n",
    "    stage_typecaster = FeatureTypeCaster(impute_columns)\n",
    "\n",
    "    # Stage where coulmns are imputed if they have NAs\n",
    "    stage_imputer = Imputer(inputCols=impute_columns, outputCols=impute_columns)\n",
    "\n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols=nominal_cols, outputCols=nominal_id_cols)\n",
    "\n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "\n",
    "    # Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "    feature_cols = impute_columns+nominal_onehot_cols\n",
    "    for col_name in columns_to_drop:\n",
    "        if col_name in nominal_cols:\n",
    "            feature_cols.remove(col_name+\"_encoded\")\n",
    "            feature_cols.remove(col_name+\"_index\")\n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "\n",
    "    # Stage for creating the outcome column representing whether there is attack \n",
    "    stage_outcome = OutcomeCreater()\n",
    "\n",
    "    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols + nominal_id_cols + nominal_onehot_cols +\n",
    "        binary_cols + continuous_cols + columns_to_drop + ['vectorized_features'])\n",
    "    # stage_column_dropper = ColumnDropper(columns_to_drop = nominal_id_cols + columns_to_drop + ['vectorized_features'])\n",
    "    # Connect the columns into a pipeline\n",
    "    pipeline = Pipeline(stages=[stage_typecaster, stage_imputer, stage_nominal_indexer, stage_nominal_onehot_encoder,\n",
    "        stage_vector_assembler, stage_scaler, stage_outcome, stage_column_dropper])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad9e78-144a-44bb-80a0-163113755380",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = get_preprocess_pipeline()\n",
    "preprocess_pipeline_model = preprocess_pipeline.fit(mqtt_raw)\n",
    "\n",
    "mqtt_df = preprocess_pipeline_model.transform(mqtt_raw)\n",
    "mqtt_df_test = preprocess_pipeline_model.transform(mqtt_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16270dcf-1b3e-4d93-adc7-ded87b584be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqtt_df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83acfd-cb1d-44c2-b77c-36d16be414e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqtt_df_test.show(3, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e66c9-50b0-40af-ab3f-3c4e38bd8900",
   "metadata": {},
   "source": [
    "<h1>Pyspark ML Models</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pyspark ml model 1 - Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome')\n",
    "lrModel = lr.fit(mqtt_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pyspark ml model 2 - Random Forest Decision Tree\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(featuresCol = 'features', labelCol = 'outcome')\n",
    "rf_model = rfc.fit(mqtt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30e7bb-acc8-4780-a83e-8f1566384f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_LR = lrModel.transform(mqtt_df_test)\n",
    "y_pred = pred_LR.select('prediction').collect()\n",
    "y = mqtt_df_test.select('outcome').collect()\n",
    "correct = 0\n",
    "for i in range(len(y)):\n",
    "    if y[i][0]==y_pred[i][0]:\n",
    "        correct += 1\n",
    "print(\"Linear Regression Test Accuracy:\",correct/len(y)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb57f7a-e526-42fa-873c-143ed9a4f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_DT = rf_model.transform(mqtt_df_test)\n",
    "y_pred_DT = pred_DT.select('prediction').collect()\n",
    "correct = 0\n",
    "for i in range(len(y)):\n",
    "    if y[i][0]==y_pred_DT[i][0]:\n",
    "        correct += 1\n",
    "print(\"Decision Tree Test Accuracy:\",correct/len(y)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc926c2f-e886-4c2d-b5db-b6387cc712a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Hyperparameters for Linear Regression\n",
    "lr_paramGrid = (ParamGridBuilder()\n",
    "                .addGrid(lr.regParam, [0.005, 0.01, 0.05])\n",
    "                .addGrid(lr.maxIter, [5, 10])\n",
    "                .build())\n",
    "\n",
    "# Hyperparameters for Random Forest\n",
    "rf_paramGrid = (ParamGridBuilder()\n",
    "                .addGrid(rfc.maxDepth, [5, 10])\n",
    "                .addGrid(rfc.numTrees, [10, 20, 40])\n",
    "                .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction',\n",
    "                                        labelCol='outcome',\n",
    "                                        metricName='accuracy')\n",
    "\n",
    "lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=lr_paramGrid,\n",
    "                       evaluator=evaluator, numFolds=5)\n",
    "rf_cv = CrossValidator(estimator=rfc, estimatorParamMaps=rf_paramGrid,\n",
    "                       evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d42346-d2c9-41c9-901e-938b07042029",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_model = lr_cv.fit(mqtt_df)\n",
    "lr_cv_prediction_test = lr_cv_model.transform(mqtt_df_test)\n",
    "print('Linear Regression Hyperparameter tuning results: ')\n",
    "print('Test Area under ROC (AUC) before Cross-Validation:', evaluator.evaluate(lr_test_predictions))\n",
    "print('Test Area under ROC (AUC) after Cross-Validation:', evaluator.evaluate(lr_cv_prediction_test))\n",
    "print()\n",
    "\n",
    "rf_cv_model = rf_cv.fit(mqtt_df)\n",
    "rf_cv_prediction_test = rf_cv_model.transform(mqtt_df_test)\n",
    "print('Random Forest Hyperparameter tuning results: ')\n",
    "print('Test Area under ROC (AUC) before Cross-Validation:', evaluator.evaluate(rfc_test_predictions))\n",
    "print('Test Area under ROC (AUC) after Cross-Validation:', evaluator.evaluate(rf_cv_prediction_test))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af7fbed-a0b9-40ef-af16-d5300a4ae96b",
   "metadata": {},
   "source": [
    "<h1>Pytorch Learning</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6eb6c-147c-43cb-ac22-21970c7d9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4f827-b10c-4d6e-a40b-21617b470df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqtt_df_val, mqtt_df_test = mqtt_df_test.randomSplit([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c020bfa-ac00-4109-9e3f-470cc2aee771",
   "metadata": {},
   "outputs": [],
   "source": [
    "mqtt_df_val_pd = mqtt_df_val.toPandas()\n",
    "mqtt_df_train_pd = mqtt_df.toPandas()\n",
    "mqtt_df_test_pd = mqtt_df_test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e49686-e204-4a96-85c2-2c20cb62e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(np.array(mqtt_df_train_pd['features'].values.tolist(),np.float32))\n",
    "y_train = torch.from_numpy(np.array(mqtt_df_train_pd['outcome'].values.tolist(),np.int64))\n",
    "\n",
    "x_test = torch.from_numpy(np.array(mqtt_df_test_pd['features'].values.tolist(),np.float32))\n",
    "y_test = torch.from_numpy(np.array(mqtt_df_test_pd['outcome'].values.tolist(),np.int64))\n",
    "\n",
    "x_val = torch.from_numpy(np.array(mqtt_df_val_pd['features'].values.tolist(),np.float32))\n",
    "y_val = torch.from_numpy(np.array(mqtt_df_val_pd['outcome'].values.tolist(),np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457e701a-cd7c-4f50-b24b-4000e8094a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return(self.x[idx],self.y[idx])\n",
    "\n",
    "train_dataset = MyDataset(x_train,y_train)\n",
    "validate_dataset = MyDataset(x_val,y_val)\n",
    "test_dataset = MyDataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c1095-7a46-4965-a1cf-2e5f314d721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size=2, output_size=1, hidden_layer_sizes=[24,24], activation_func = nn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []                                               # List of layers\n",
    "        layers.append(nn.Linear(input_size,hidden_layer_sizes[0]))         # First hidden layer\n",
    "\n",
    "                                                                  # Next hidden layers\n",
    "        for i in range(len(hidden_layer_sizes)-1):\n",
    "            layers.append(activation_func())\n",
    "            layers.append(nn.Linear(hidden_layer_sizes[i],hidden_layer_sizes[i+1]))\n",
    "            \n",
    "        layers.append(activation_func())                                  # Output layer (if needed)\n",
    "        layers.append(nn.Linear(hidden_layer_sizes[-1], output_size))\n",
    "        \n",
    "        self.seq = nn.Sequential(*layers)                         # Final sequential model\n",
    "\n",
    "    def forward(self, xy):\n",
    "        return self.seq(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54136f93-1515-4c5e-8dc0-317464092244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, lr = 0.01, epochs = 1000, batch_size=32, gamma=1):\n",
    "    # Define loss, optimizer, and scheduler\n",
    "    lossfun = nn.functional.cross_entropy\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer=opt, gamma=gamma)\n",
    "\n",
    "    # Define data loaders to do batch decent\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    train_loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    batch_loss_hist = []\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs+1):\n",
    "\n",
    "        # Training batch loop for this epoch\n",
    "        train_batch_loss=[]\n",
    "        train_batch_acc=[]\n",
    "        model.train()\n",
    "        for _, (x_batch, y_batch) in enumerate(train_dataloader):\n",
    "            test_batch_pred = model(x_batch)\n",
    "            loss_train = lossfun(test_batch_pred, y_batch)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss_train.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_batch_loss.append(loss_train.detach())\n",
    "            test_batch_label = np.argmax(test_batch_pred.detach(), axis=1).numpy()\n",
    "            train_batch_acc.append(np.sum(test_batch_label == y_batch.numpy())/y_batch.shape[0])\n",
    "\n",
    "        # Validation batch loop for this epoch\n",
    "        val_batch_loss = []\n",
    "        val_batch_acc=[]\n",
    "        model.eval()\n",
    "        for _, (x_batch_val, y_batch_val) in enumerate(val_dataloader):\n",
    "            val_batch_pred = model(x_batch_val)\n",
    "            loss_val = lossfun(val_batch_pred, y_batch_val)\n",
    "\n",
    "            val_batch_loss.append(loss_val.detach())\n",
    "            val_batch_label = np.argmax(val_batch_pred.detach(), axis=1).numpy()\n",
    "            val_batch_acc.append(np.sum(val_batch_label == y_batch_val.numpy())/y_batch_val.shape[0])\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save data to histories\n",
    "        batch_loss_hist += train_batch_loss\n",
    "        train_loss_hist.append(np.mean(train_batch_loss))\n",
    "        val_loss_hist.append(np.mean(val_batch_loss))\n",
    "        train_acc_hist.append(np.mean(train_batch_acc))\n",
    "        val_acc_hist.append(np.mean(val_batch_acc))\n",
    "\n",
    "        # Print epoch info\n",
    "        if epoch%int(epochs/10) == 0:\n",
    "            print(f\"Epoch {epoch:>4} of {epochs}:   Train Loss = {   train_loss_hist[-1]:.3f}   Validation Loss = {   val_loss_hist[-1]:.3f}\"+\n",
    "                                               f\"   Train Acc  = {100*train_acc_hist[-1]:.3f}   Validation Acc  = {100*val_acc_hist[-1]:.3f}\")\n",
    "\n",
    "        # Save model if it is the best so far\n",
    "        if val_acc_hist[-1] > best_acc:\n",
    "            torch.save(model.state_dict(), 'current_best_model')\n",
    "            best_acc = val_acc_hist[-1]\n",
    "\n",
    "    plt.figure(figsize=(4,2),dpi=250)\n",
    "    plt.plot(batch_loss_hist)\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss across all batches\")\n",
    "    plt.show()\n",
    "\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    axs[0].plot(train_loss_hist,label=\"Training\")\n",
    "    axs[0].plot(val_loss_hist,label=\"Validation\")\n",
    "    axs[0].legend()\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].set_title(\"Loss across Epochs\")\n",
    "\n",
    "    axs[1].plot(train_acc_hist,label=\"Training\")\n",
    "    axs[1].plot(val_acc_hist,label=\"Validation\")\n",
    "    axs[1].legend()\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].set_title(\"Accuracy across Epochs\")\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773c70a3-21c2-43a5-b040-999f529e8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch model #1 - Deep Nueral Network\n",
    "hidden_layers = [128,128,128,128]\n",
    "activation = nn.RReLU\n",
    "net = NeuralNet(input_size=x_train.shape[1], output_size=6, hidden_layer_sizes=hidden_layers, activation_func=activation)\n",
    "print(net)\n",
    "train(net, train_dataset, validate_dataset, lr=0.005, epochs=20, batch_size=64, gamma=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b14a1e8-6a16-4da3-b0fd-4fee35235ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = NeuralNet(input_size=x_train.shape[1], output_size=6, hidden_layer_sizes=hidden_layers, activation_func=activation)\n",
    "best_model.load_state_dict(torch.load(\"current_best_model\"))\n",
    "\n",
    "prediction = np.argmax(best_model(x_test).detach(), axis=1).numpy()\n",
    "accuracy = 100*np.sum(prediction == y_test.numpy())/y_test.shape[0]\n",
    "print(f\"Testing accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c3705f-e1b4-44ec-8bff-1b74e4f0622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch model #2 - Shallow Nueral Network\n",
    "hidden_layers = [8,8]\n",
    "activation = nn.ReLU\n",
    "net_shallow = NeuralNet(input_size=x_train.shape[1], output_size=6, hidden_layer_sizes=hidden_layers, activation_func=activation)\n",
    "print(net_shallow)\n",
    "train(net_shallow, train_dataset, validate_dataset, lr=0.05, epochs=10, batch_size=64, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db285573-de10-4d9b-8b08-fb5da496a91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_shallow = NeuralNet(input_size=x_train.shape[1], output_size=6, hidden_layer_sizes=hidden_layers, activation_func=activation)\n",
    "best_model_shallow.load_state_dict(torch.load(\"current_best_model\"))\n",
    "\n",
    "prediction_shallow = np.argmax(best_model_shallow(x_test).detach(), axis=1).numpy()\n",
    "accuracy_shallow = 100*np.sum(prediction_shallow == y_test.numpy())/y_test.shape[0]\n",
    "print(f\"Shallow network testing accuracy: {accuracy_shallow:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
