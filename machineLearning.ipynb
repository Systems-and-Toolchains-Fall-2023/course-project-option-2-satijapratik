{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cd682ae-d094-4bae-84fe-2e3f10aa9893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import col, SparkContext, udf\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589cccdf-5556-40ea-a306-484322f56f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Woodw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "appName = \"Big Data ML\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "conf = pyspark.SparkConf()\\\n",
    "    .set('spark.driver.host','127.0.0.1')\\\n",
    "    .setAppName(appName)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "# Create Spark Context with the new configurations rather than relying on the default one\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# You need to create SQL Context to conduct some database operations like what we will see later.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# If you have SQL context, you create the session from the Spark Context\n",
    "# spark = sqlContext.sparkSession.builder.getOrCreate()\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f531d3-81fe-413d-b5ba-cae974854464",
   "metadata": {},
   "outputs": [],
   "source": [
    "nslkdd_raw = spark.read.csv(\"train70_reduced.csv\",header=True, inferSchema= True)\n",
    "nslkdd_test_raw = spark.read.csv(\"test30_reduced.csv\",header=True, inferSchema= True)\n",
    "\n",
    "# Rename columns\n",
    "for column_name in nslkdd_raw.columns:\n",
    "    new_column_name = column_name.replace(\".\", \"_\")\n",
    "    nslkdd_raw = nslkdd_raw.withColumnRenamed(column_name, new_column_name)\n",
    "\n",
    "for column_name in nslkdd_test_raw.columns:\n",
    "    new_column_name = column_name.replace(\".\", \"_\")\n",
    "    nslkdd_test_raw = nslkdd_test_raw.withColumnRenamed(column_name, new_column_name)\n",
    "\n",
    "# Drop the few rows with NA values (only 190)\n",
    "nslkdd_raw = nslkdd_raw.na.drop()\n",
    "nslkdd_test_raw = nslkdd_test_raw.na.drop()\n",
    "\n",
    "# Dropping a couple useless columns\n",
    "nslkdd_raw = nslkdd_raw.drop(\"mqtt_protoname\")\n",
    "nslkdd_test_raw = nslkdd_test_raw.drop(\"mqtt_protoname\")\n",
    "\n",
    "# Converting nominal columns to string so they can be encoded by the transformer\n",
    "nslkdd_raw = nslkdd_raw.withColumn(\"mqtt_msgtype\", nslkdd_raw[\"mqtt_msgtype\"].cast(\"string\"))\n",
    "nslkdd_raw = nslkdd_raw.withColumn(\"mqtt_conack_val\", nslkdd_raw[\"mqtt_conack_val\"].cast(\"string\"))\n",
    "nslkdd_test_raw = nslkdd_test_raw.withColumn(\"mqtt_msgtype\", nslkdd_test_raw[\"mqtt_msgtype\"].cast(\"string\"))\n",
    "nslkdd_test_raw = nslkdd_test_raw.withColumn(\"mqtt_conack_val\", nslkdd_test_raw[\"mqtt_conack_val\"].cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9e8735c-24c2-480a-b3fc-206551fe1504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tcp_flags: string (nullable = true)\n",
      " |-- tcp_time_delta: double (nullable = true)\n",
      " |-- tcp_len: integer (nullable = true)\n",
      " |-- mqtt_conack_flags: string (nullable = true)\n",
      " |-- mqtt_conack_flags_reserved: double (nullable = true)\n",
      " |-- mqtt_conack_flags_sp: double (nullable = true)\n",
      " |-- mqtt_conack_val: string (nullable = true)\n",
      " |-- mqtt_conflag_cleansess: double (nullable = true)\n",
      " |-- mqtt_conflag_passwd: double (nullable = true)\n",
      " |-- mqtt_conflag_qos: double (nullable = true)\n",
      " |-- mqtt_conflag_reserved: double (nullable = true)\n",
      " |-- mqtt_conflag_retain: double (nullable = true)\n",
      " |-- mqtt_conflag_uname: double (nullable = true)\n",
      " |-- mqtt_conflag_willflag: double (nullable = true)\n",
      " |-- mqtt_conflags: string (nullable = true)\n",
      " |-- mqtt_dupflag: double (nullable = true)\n",
      " |-- mqtt_hdrflags: string (nullable = true)\n",
      " |-- mqtt_kalive: double (nullable = true)\n",
      " |-- mqtt_len: double (nullable = true)\n",
      " |-- mqtt_msg: string (nullable = true)\n",
      " |-- mqtt_msgid: double (nullable = true)\n",
      " |-- mqtt_msgtype: string (nullable = true)\n",
      " |-- mqtt_proto_len: double (nullable = true)\n",
      " |-- mqtt_qos: double (nullable = true)\n",
      " |-- mqtt_retain: double (nullable = true)\n",
      " |-- mqtt_sub_qos: double (nullable = true)\n",
      " |-- mqtt_suback_qos: double (nullable = true)\n",
      " |-- mqtt_ver: double (nullable = true)\n",
      " |-- mqtt_willmsg: double (nullable = true)\n",
      " |-- mqtt_willmsg_len: double (nullable = true)\n",
      " |-- mqtt_willtopic: double (nullable = true)\n",
      " |-- mqtt_willtopic_len: double (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nslkdd_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4be9718-6e09-46cd-a3e9-05a6b3ede717",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names =  [\"tcp_flags\",\"tcp_time_delta\",\"tcp_len\",\n",
    "\"mqtt_conack_flags\",\"mqtt_conack_flags_reserved\",\"mqtt_conack_flags_sp\",\n",
    "\"mqtt_conack_val\",\"mqtt_conflag_cleansess\",\"mqtt_conflag_passwd\",\n",
    "\"mqtt_conflag_qos\",\"mqtt_conflag_reserved\",\"mqtt_conflag_retain\",\n",
    "\"mqtt_conflag_uname\",\"mqtt_conflag_willflag\",\n",
    "\"mqtt_conflags\",\"mqtt_dupflag\",\"mqtt_hdrflags\",\n",
    "\"mqtt_kalive\",\"mqtt_len\",\"mqtt_msg\",\"mqtt_msgid\",\"mqtt_msgtype\",\n",
    "\"mqtt_proto_len\",\"mqtt_qos\",\n",
    "\"mqtt_retain\",\"mqtt_sub_qos\",\"mqtt_suback_qos\",\"mqtt_ver\",\n",
    "\"mqtt_willmsg\",\"mqtt_willmsg_len\",\"mqtt_willtopic\",\"mqtt_willtopic_len\",\n",
    "\"target\"]\n",
    "\n",
    "nominal_cols = [\"mqtt_msgtype\", \"mqtt_conack_val\", \"tcp_flags\", \"mqtt_conack_flags\", \"mqtt_conflags\", \"mqtt_hdrflags\"]\n",
    "binary_cols = [\"mqtt_dupflag\"]\n",
    "continuous_cols = [\"tcp_time_delta\",\"tcp_len\",\n",
    "\"mqtt_conack_flags_reserved\",\"mqtt_conack_flags_sp\",\n",
    "\"mqtt_conflag_cleansess\",\"mqtt_conflag_passwd\",\n",
    "\"mqtt_conflag_qos\",\"mqtt_conflag_reserved\",\"mqtt_conflag_retain\",\n",
    "\"mqtt_conflag_uname\",\"mqtt_conflag_willflag\",\n",
    "\"mqtt_kalive\",\"mqtt_len\",\"mqtt_msg\",\"mqtt_msgid\",\n",
    "\"mqtt_proto_len\",\"mqtt_qos\",\n",
    "\"mqtt_retain\",\"mqtt_sub_qos\",\"mqtt_suback_qos\",\"mqtt_ver\",\n",
    "\"mqtt_willmsg\",\"mqtt_willmsg_len\",\"mqtt_willtopic\",\"mqtt_willtopic_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0619e7c8-c15c-4e82-850c-0b166617cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nslkdd_raw.select(\"mqtt_hdrflags\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ee95a4-ceff-4ec8-9a9d-d8b84cedf9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [] # Add any columns that we want to just get rid of here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d960f5-59bd-4901-a8d5-a02777941b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data preprocessing pipeline\n",
    "'''\n",
    "class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def label_to_vector(self, name):\n",
    "        name = name.lower()\n",
    "        \n",
    "        if name == 'normal':\n",
    "            return 0.0\n",
    "        elif name == 'dos':\n",
    "            return 1.0\n",
    "        elif name == 'malformed':\n",
    "            return 2.0\n",
    "        elif name == 'slowite':\n",
    "            return 3.0\n",
    "        elif name == 'bruteforce':\n",
    "            return 4.0\n",
    "        else:\n",
    "            return -100.0\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        label_to_vector_udf = udf(self.label_to_vector, StringType())\n",
    "        output_df = dataset.withColumn('outcome', label_to_vector_udf(col('target'))).drop(\"target\")  \n",
    "        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "        return output_df\n",
    "\n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in binary_cols + continuous_cols:\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
    "\n",
    "        return output_df\n",
    "\n",
    "class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "def get_preprocess_pipeline():\n",
    "    # Stage where columns are casted as appropriate types\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "    # Stage where coulmns are imputed if they have NAs\n",
    "    stage_imputer = Imputer(inputCols=binary_cols+continuous_cols, outputCols=binary_cols+continuous_cols)\n",
    "\n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols=nominal_cols, outputCols=nominal_id_cols)\n",
    "\n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "\n",
    "    # Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "    feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n",
    "    for col_name in columns_to_drop:\n",
    "        if col_name in nominal_cols:\n",
    "            feature_cols.remove(col_name+\"_encoded\")\n",
    "        else:\n",
    "            feature_cols.remove(col_name)\n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "\n",
    "    # Stage for creating the outcome column representing whether there is attack \n",
    "    stage_outcome = OutcomeCreater()\n",
    "\n",
    "    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols + nominal_id_cols + nominal_onehot_cols +\n",
    "        binary_cols + continuous_cols + columns_to_drop + ['vectorized_features'])\n",
    "    # Connect the columns into a pipeline\n",
    "    pipeline = Pipeline(stages=[stage_typecaster, stage_imputer, stage_nominal_indexer, stage_nominal_onehot_encoder,\n",
    "        stage_vector_assembler, stage_scaler, stage_outcome, stage_column_dropper])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baad9e78-144a-44bb-80a0-163113755380",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = get_preprocess_pipeline()\n",
    "preprocess_pipeline_model = preprocess_pipeline.fit(nslkdd_raw)\n",
    "\n",
    "nslkdd_df = preprocess_pipeline_model.transform(nslkdd_raw)\n",
    "nslkdd_df_test = preprocess_pipeline_model.transform(nslkdd_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16270dcf-1b3e-4d93-adc7-ded87b584be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- outcome: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nslkdd_df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e83acfd-cb1d-44c2-b77c-36d16be414e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------\n",
      " features | (61,[0,27,36,44,4... \n",
      " outcome  | 2.0                  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nslkdd_df_test.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bccab5-3a90-49ad-a9da-4dc35202fd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
