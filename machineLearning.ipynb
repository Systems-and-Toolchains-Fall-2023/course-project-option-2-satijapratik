{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cd682ae-d094-4bae-84fe-2e3f10aa9893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import col, SparkContext, udf\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "589cccdf-5556-40ea-a306-484322f56f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\python\\pyspark\\sql\\context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "appName = \"Big Data ML\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "conf = pyspark.SparkConf()\\\n",
    "    .set('spark.driver.host','127.0.0.1')\\\n",
    "    .setAppName(appName)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "# Create Spark Context with the new configurations rather than relying on the default one\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# You need to create SQL Context to conduct some database operations like what we will see later.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# If you have SQL context, you create the session from the Spark Context\n",
    "# spark = sqlContext.sparkSession.builder.getOrCreate()\n",
    "spark = sqlContext.sparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5f531d3-81fe-413d-b5ba-cae974854464",
   "metadata": {},
   "outputs": [],
   "source": [
    "nslkdd_raw = spark.read.csv(\"train70_reduced.csv\",header=True, inferSchema= True)\n",
    "nslkdd_test_raw = spark.read.csv(\"test30_reduced.csv\",header=True, inferSchema= True)\n",
    "\n",
    "# Rename columns\n",
    "for column_name in nslkdd_raw.columns:\n",
    "    new_column_name = column_name.replace(\".\", \"_\")\n",
    "    nslkdd_raw = nslkdd_raw.withColumnRenamed(column_name, new_column_name)\n",
    "\n",
    "for column_name in nslkdd_test_raw.columns:\n",
    "    new_column_name = column_name.replace(\".\", \"_\")\n",
    "    nslkdd_test_raw = nslkdd_test_raw.withColumnRenamed(column_name, new_column_name)\n",
    "\n",
    "# Drop the few rows with NA values (only 190)\n",
    "nslkdd_raw = nslkdd_raw.na.drop()\n",
    "nslkdd_test_raw = nslkdd_test_raw.na.drop()\n",
    "\n",
    "# Dropping a couple useless columns\n",
    "nslkdd_raw = nslkdd_raw.drop(\"mqtt_protoname\")\n",
    "nslkdd_test_raw = nslkdd_test_raw.drop(\"mqtt_protoname\")\n",
    "\n",
    "# Converting nominal columns to string so they can be encoded by the transformer \n",
    "nslkdd_raw = nslkdd_raw.withColumn(\"mqtt_msgtype\", nslkdd_raw[\"mqtt_msgtype\"].cast(\"string\"))\n",
    "nslkdd_raw = nslkdd_raw.withColumn(\"mqtt_conack_val\", nslkdd_raw[\"mqtt_conack_val\"].cast(\"string\"))\n",
    "nslkdd_test_raw = nslkdd_test_raw.withColumn(\"mqtt_msgtype\", nslkdd_test_raw[\"mqtt_msgtype\"].cast(\"string\"))\n",
    "nslkdd_test_raw = nslkdd_test_raw.withColumn(\"mqtt_conack_val\", nslkdd_test_raw[\"mqtt_conack_val\"].cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9e8735c-24c2-480a-b3fc-206551fe1504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tcp_flags: string (nullable = true)\n",
      " |-- tcp_time_delta: double (nullable = true)\n",
      " |-- tcp_len: integer (nullable = true)\n",
      " |-- mqtt_conack_flags: string (nullable = true)\n",
      " |-- mqtt_conack_flags_reserved: double (nullable = true)\n",
      " |-- mqtt_conack_flags_sp: double (nullable = true)\n",
      " |-- mqtt_conack_val: double (nullable = true)\n",
      " |-- mqtt_conflag_cleansess: double (nullable = true)\n",
      " |-- mqtt_conflag_passwd: double (nullable = true)\n",
      " |-- mqtt_conflag_qos: double (nullable = true)\n",
      " |-- mqtt_conflag_reserved: double (nullable = true)\n",
      " |-- mqtt_conflag_retain: double (nullable = true)\n",
      " |-- mqtt_conflag_uname: double (nullable = true)\n",
      " |-- mqtt_conflag_willflag: double (nullable = true)\n",
      " |-- mqtt_conflags: string (nullable = true)\n",
      " |-- mqtt_dupflag: double (nullable = true)\n",
      " |-- mqtt_hdrflags: string (nullable = true)\n",
      " |-- mqtt_kalive: double (nullable = true)\n",
      " |-- mqtt_len: double (nullable = true)\n",
      " |-- mqtt_msg: string (nullable = true)\n",
      " |-- mqtt_msgid: double (nullable = true)\n",
      " |-- mqtt_msgtype: double (nullable = true)\n",
      " |-- mqtt_proto_len: double (nullable = true)\n",
      " |-- mqtt_qos: double (nullable = true)\n",
      " |-- mqtt_retain: double (nullable = true)\n",
      " |-- mqtt_sub_qos: double (nullable = true)\n",
      " |-- mqtt_suback_qos: double (nullable = true)\n",
      " |-- mqtt_ver: double (nullable = true)\n",
      " |-- mqtt_willmsg: double (nullable = true)\n",
      " |-- mqtt_willmsg_len: double (nullable = true)\n",
      " |-- mqtt_willtopic: double (nullable = true)\n",
      " |-- mqtt_willtopic_len: double (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nslkdd_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4be9718-6e09-46cd-a3e9-05a6b3ede717",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names =  [\"tcp_flags\",\"tcp_time_delta\",\"tcp_len\",\n",
    "\"mqtt_conack_flags\",\"mqtt_conack_flags_reserved\",\"mqtt_conack_flags_sp\",\n",
    "\"mqtt_conack_val\",\"mqtt_conflag_cleansess\",\"mqtt_conflag_passwd\",\n",
    "\"mqtt_conflag_qos\",\"mqtt_conflag_reserved\",\"mqtt_conflag_retain\",\n",
    "\"mqtt_conflag_uname\",\"mqtt_conflag_willflag\",\n",
    "\"mqtt_conflags\",\"mqtt_dupflag\",\"mqtt_hdrflags\",\n",
    "\"mqtt_kalive\",\"mqtt_len\",\"mqtt_msg\",\"mqtt_msgid\",\"mqtt_msgtype\",\n",
    "\"mqtt_proto_len\",\"mqtt_qos\",\n",
    "\"mqtt_retain\",\"mqtt_sub_qos\",\"mqtt_suback_qos\",\"mqtt_ver\",\n",
    "\"mqtt_willmsg\",\"mqtt_willmsg_len\",\"mqtt_willtopic\",\"mqtt_willtopic_len\",\n",
    "\"target\"]\n",
    "\n",
    "nominal_cols = [\"mqtt_msgtype\", \"mqtt_conack_val\"]\n",
    "binary_cols = [\"mqtt_dupflag\"]\n",
    "continuous_cols = [\"tcp_time_delta\",\"tcp_len\",\n",
    "\"mqtt_conack_flags_reserved\",\"mqtt_conack_flags_sp\",\n",
    "\"mqtt_conflag_cleansess\",\"mqtt_conflag_passwd\",\n",
    "\"mqtt_conflag_qos\",\"mqtt_conflag_reserved\",\"mqtt_conflag_retain\",\n",
    "\"mqtt_conflag_uname\",\"mqtt_conflag_willflag\",\n",
    "\"mqtt_kalive\",\"mqtt_len\",\"mqtt_msg\",\"mqtt_msgid\",\n",
    "\"mqtt_proto_len\",\"mqtt_qos\",\n",
    "\"mqtt_retain\",\"mqtt_sub_qos\",\"mqtt_suback_qos\",\"mqtt_ver\",\n",
    "\"mqtt_willmsg\",\"mqtt_willmsg_len\",\"mqtt_willtopic\",\"mqtt_willtopic_len\", \"tcp_flags\", \"mqtt_conack_flags\", \"mqtt_conflags\", \"mqtt_hdrflags\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0619e7c8-c15c-4e82-850c-0b166617cc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|mqtt_conack_val|\n",
      "+---------------+\n",
      "|            0.0|\n",
      "|            5.0|\n",
      "+---------------+\n",
      "\n",
      "+------------+\n",
      "|mqtt_msgtype|\n",
      "+------------+\n",
      "|         1.0|\n",
      "|         0.0|\n",
      "|         9.0|\n",
      "|        12.0|\n",
      "|        14.0|\n",
      "|         5.0|\n",
      "|         4.0|\n",
      "|         2.0|\n",
      "|         8.0|\n",
      "|         3.0|\n",
      "|        13.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nslkdd_raw.select(\"mqtt_conack_val\").distinct().show()\n",
    "nslkdd_raw.select(\"mqtt_msgtype\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69ee95a4-ceff-4ec8-9a9d-d8b84cedf9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not sure if we should drop \"mqtt_hdrflags\"\n",
    "# columns_to_drop = [\"mqtt_conack_flags\", \"mqtt_conflags\", \"mqtt_hdrflags\"] # Add any columns that we want to just get rid of here\n",
    "columns_to_drop = [\"mqtt_conack_flags\", \"mqtt_conflags\", \"mqtt_hdrflags\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f955650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " tcp_flags                  | 0   \n",
      " tcp_time_delta             | 0   \n",
      " tcp_len                    | 0   \n",
      " mqtt_conack_flags          | 0   \n",
      " mqtt_conack_flags_reserved | 0   \n",
      " mqtt_conack_flags_sp       | 0   \n",
      " mqtt_conack_val            | 0   \n",
      " mqtt_conflag_cleansess     | 0   \n",
      " mqtt_conflag_passwd        | 0   \n",
      " mqtt_conflag_qos           | 0   \n",
      " mqtt_conflag_reserved      | 0   \n",
      " mqtt_conflag_retain        | 0   \n",
      " mqtt_conflag_uname         | 0   \n",
      " mqtt_conflag_willflag      | 0   \n",
      " mqtt_conflags              | 0   \n",
      " mqtt_dupflag               | 0   \n",
      " mqtt_hdrflags              | 0   \n",
      " mqtt_kalive                | 0   \n",
      " mqtt_len                   | 0   \n",
      " mqtt_msg                   | 0   \n",
      " mqtt_msgid                 | 0   \n",
      " mqtt_msgtype               | 0   \n",
      " mqtt_proto_len             | 0   \n",
      " mqtt_qos                   | 0   \n",
      " mqtt_retain                | 0   \n",
      " mqtt_sub_qos               | 0   \n",
      " mqtt_suback_qos            | 0   \n",
      " mqtt_ver                   | 0   \n",
      " mqtt_willmsg               | 0   \n",
      " mqtt_willmsg_len           | 0   \n",
      " mqtt_willtopic             | 0   \n",
      " mqtt_willtopic_len         | 0   \n",
      " target                     | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "null_counts_plays_df = nslkdd_raw.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \\\n",
    "                                               for c in nslkdd_raw.columns])\n",
    "\n",
    "null_counts_plays_df.show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fb9ab6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tcp_flags: integer (nullable = true)\n",
      " |-- tcp_time_delta: double (nullable = true)\n",
      " |-- tcp_len: integer (nullable = true)\n",
      " |-- mqtt_conack_flags: string (nullable = true)\n",
      " |-- mqtt_conack_flags_reserved: double (nullable = true)\n",
      " |-- mqtt_conack_flags_sp: double (nullable = true)\n",
      " |-- mqtt_conack_val: string (nullable = true)\n",
      " |-- mqtt_conflag_cleansess: double (nullable = true)\n",
      " |-- mqtt_conflag_passwd: double (nullable = true)\n",
      " |-- mqtt_conflag_qos: double (nullable = true)\n",
      " |-- mqtt_conflag_reserved: double (nullable = true)\n",
      " |-- mqtt_conflag_retain: double (nullable = true)\n",
      " |-- mqtt_conflag_uname: double (nullable = true)\n",
      " |-- mqtt_conflag_willflag: double (nullable = true)\n",
      " |-- mqtt_conflags: string (nullable = true)\n",
      " |-- mqtt_dupflag: double (nullable = true)\n",
      " |-- mqtt_hdrflags: string (nullable = true)\n",
      " |-- mqtt_kalive: double (nullable = true)\n",
      " |-- mqtt_len: double (nullable = true)\n",
      " |-- mqtt_msg: string (nullable = true)\n",
      " |-- mqtt_msgid: double (nullable = true)\n",
      " |-- mqtt_msgtype: string (nullable = true)\n",
      " |-- mqtt_proto_len: double (nullable = true)\n",
      " |-- mqtt_qos: double (nullable = true)\n",
      " |-- mqtt_retain: double (nullable = true)\n",
      " |-- mqtt_sub_qos: double (nullable = true)\n",
      " |-- mqtt_suback_qos: double (nullable = true)\n",
      " |-- mqtt_ver: double (nullable = true)\n",
      " |-- mqtt_willmsg: double (nullable = true)\n",
      " |-- mqtt_willmsg_len: double (nullable = true)\n",
      " |-- mqtt_willtopic: double (nullable = true)\n",
      " |-- mqtt_willtopic_len: double (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting hex to Decimal for tcp_flags, mqtt_conack_flags, mqtt_conflags, mqtt_hdrflags\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "nslkdd_raw = nslkdd_raw.withColumn('tcp_flags', F.expr(\"int(conv(substring(tcp_flags,3),16,10))\"))\n",
    "# nslkdd_raw = nslkdd_raw.drop('mqtt_conack_flags', 'mqtt_hdrflags', 'mqtt_conflags')\n",
    "# nslkdd_raw = nslkdd_raw.withColumn('mqtt_conack_flags', F.expr(\"int(conv(substring(mqtt_conack_flags,3),16,10))\"))\n",
    "# nslkdd_raw = nslkdd_raw.withColumn('mqtt_hdrflags', F.expr(\"int(conv(substring(mqtt_hdrflags,3),16,10))\"))\n",
    "# nslkdd_raw = nslkdd_raw.withColumn('mqtt_conflags', F.expr(\"int(conv(substring(mqtt_conflags,3),16,10))\"))\n",
    "nslkdd_raw.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2489fcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " tcp_flags                  | 0   \n",
      " tcp_time_delta             | 0   \n",
      " tcp_len                    | 0   \n",
      " mqtt_conack_flags          | 0   \n",
      " mqtt_conack_flags_reserved | 0   \n",
      " mqtt_conack_flags_sp       | 0   \n",
      " mqtt_conack_val            | 0   \n",
      " mqtt_conflag_cleansess     | 0   \n",
      " mqtt_conflag_passwd        | 0   \n",
      " mqtt_conflag_qos           | 0   \n",
      " mqtt_conflag_reserved      | 0   \n",
      " mqtt_conflag_retain        | 0   \n",
      " mqtt_conflag_uname         | 0   \n",
      " mqtt_conflag_willflag      | 0   \n",
      " mqtt_conflags              | 0   \n",
      " mqtt_dupflag               | 0   \n",
      " mqtt_hdrflags              | 0   \n",
      " mqtt_kalive                | 0   \n",
      " mqtt_len                   | 0   \n",
      " mqtt_msg                   | 0   \n",
      " mqtt_msgid                 | 0   \n",
      " mqtt_msgtype               | 0   \n",
      " mqtt_proto_len             | 0   \n",
      " mqtt_qos                   | 0   \n",
      " mqtt_retain                | 0   \n",
      " mqtt_sub_qos               | 0   \n",
      " mqtt_suback_qos            | 0   \n",
      " mqtt_ver                   | 0   \n",
      " mqtt_willmsg               | 0   \n",
      " mqtt_willmsg_len           | 0   \n",
      " mqtt_willtopic             | 0   \n",
      " mqtt_willtopic_len         | 0   \n",
      " target                     | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "null_counts_plays_df_new = nslkdd_raw\n",
    "null_counts_plays_df = null_counts_plays_df_new.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \\\n",
    "                                               for c in null_counts_plays_df_new.columns])\n",
    "\n",
    "null_counts_plays_df.show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48d960f5-59bd-4901-a8d5-a02777941b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data preprocessing pipeline\n",
    "'''\n",
    "class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def label_to_vector(self, name):\n",
    "        name = name.lower()\n",
    "        \n",
    "        if name == 'legitimate':\n",
    "            return 0.0\n",
    "        elif name == 'dos':\n",
    "            return 1.0\n",
    "        elif name == 'malformed':\n",
    "            return 2.0\n",
    "        elif name == 'slowite':\n",
    "            return 3.0\n",
    "        elif name == 'bruteforce':\n",
    "            return 4.0\n",
    "        elif name == 'flood':\n",
    "            return 5.0\n",
    "        else:\n",
    "            return -100.0\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        label_to_vector_udf = udf(self.label_to_vector, StringType())\n",
    "        output_df = dataset.withColumn('outcome', label_to_vector_udf(col('target'))).drop(\"target\")  \n",
    "        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n",
    "        return output_df\n",
    "\n",
    "class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in binary_cols + continuous_cols:\n",
    "            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n",
    "\n",
    "        return output_df\n",
    "\n",
    "class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n",
    "    def __init__(self, columns_to_drop = None):\n",
    "        super().__init__()\n",
    "        self.columns_to_drop=columns_to_drop\n",
    "    def _transform(self, dataset):\n",
    "        output_df = dataset\n",
    "        for col_name in self.columns_to_drop:\n",
    "            output_df = output_df.drop(col_name)\n",
    "        return output_df\n",
    "\n",
    "def get_preprocess_pipeline():\n",
    "    # Stage where columns are casted as appropriate types\n",
    "    stage_typecaster = FeatureTypeCaster()\n",
    "\n",
    "    # Stage where coulmns are imputed if they have NAs\n",
    "    stage_imputer = Imputer(inputCols=binary_cols+continuous_cols, outputCols=binary_cols+continuous_cols)\n",
    "\n",
    "    # Stage where nominal columns are transformed to index columns using StringIndexer\n",
    "    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n",
    "    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n",
    "    stage_nominal_indexer = StringIndexer(inputCols=nominal_cols, outputCols=nominal_id_cols)\n",
    "\n",
    "    # Stage where the index columns are further transformed using OneHotEncoder\n",
    "    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n",
    "\n",
    "    # Stage where all relevant features are assembled into a vector (and dropping a few)\n",
    "    feature_cols = continuous_cols+binary_cols+nominal_onehot_cols\n",
    "    for col_name in columns_to_drop:\n",
    "        if col_name in nominal_cols:\n",
    "            feature_cols.remove(col_name+\"_encoded\")\n",
    "        else:\n",
    "            feature_cols.remove(col_name)\n",
    "    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n",
    "\n",
    "    # Stage where we scale the columns\n",
    "    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "\n",
    "    # Stage for creating the outcome column representing whether there is attack \n",
    "    stage_outcome = OutcomeCreater()\n",
    "\n",
    "    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n",
    "    # stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols + nominal_id_cols + nominal_onehot_cols +\n",
    "    #     binary_cols + continuous_cols + columns_to_drop + ['vectorized_features'])\n",
    "    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_id_cols + columns_to_drop + ['vectorized_features'])\n",
    "    # Connect the columns into a pipeline\n",
    "    pipeline = Pipeline(stages=[stage_typecaster, stage_imputer, stage_nominal_indexer, stage_nominal_onehot_encoder,\n",
    "        stage_vector_assembler, stage_scaler, stage_outcome, stage_column_dropper])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "baad9e78-144a-44bb-80a0-163113755380",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = get_preprocess_pipeline()\n",
    "preprocess_pipeline_model = preprocess_pipeline.fit(nslkdd_raw)\n",
    "\n",
    "nslkdd_df = preprocess_pipeline_model.transform(nslkdd_raw)\n",
    "nslkdd_df_test = preprocess_pipeline_model.transform(nslkdd_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16270dcf-1b3e-4d93-adc7-ded87b584be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- outcome: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nslkdd_df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e5061fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# null_counts_plays_df_new = nslkdd_df_test.drop('features')\n",
    "# null_counts_plays_df = null_counts_plays_df_new.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \\\n",
    "#                                                for c in null_counts_plays_df_new.columns])\n",
    "\n",
    "# null_counts_plays_df.show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e83acfd-cb1d-44c2-b77c-36d16be414e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------\n",
      " tcp_flags                  | 20.295459451058942   \n",
      " tcp_time_delta             | 0.029854             \n",
      " tcp_len                    | 0.0                  \n",
      " mqtt_conack_flags_reserved | 0.0                  \n",
      " mqtt_conack_flags_sp       | 0.0                  \n",
      " mqtt_conack_val            | 0.0                  \n",
      " mqtt_conflag_cleansess     | 0.0                  \n",
      " mqtt_conflag_passwd        | 0.0                  \n",
      " mqtt_conflag_qos           | 0.0                  \n",
      " mqtt_conflag_reserved      | 0.0                  \n",
      " mqtt_conflag_retain        | 0.0                  \n",
      " mqtt_conflag_uname         | 0.0                  \n",
      " mqtt_conflag_willflag      | 0.0                  \n",
      " mqtt_dupflag               | 0.0                  \n",
      " mqtt_kalive                | 0.0                  \n",
      " mqtt_len                   | 0.0                  \n",
      " mqtt_msg                   | 0.0                  \n",
      " mqtt_msgid                 | 0.0                  \n",
      " mqtt_msgtype               | 0.0                  \n",
      " mqtt_proto_len             | 0.0                  \n",
      " mqtt_qos                   | 0.0                  \n",
      " mqtt_retain                | 0.0                  \n",
      " mqtt_sub_qos               | 0.0                  \n",
      " mqtt_suback_qos            | 0.0                  \n",
      " mqtt_ver                   | 0.0                  \n",
      " mqtt_willmsg               | 0.0                  \n",
      " mqtt_willmsg_len           | 0.0                  \n",
      " mqtt_willtopic             | 0.0                  \n",
      " mqtt_willtopic_len         | 0.0                  \n",
      " mqtt_msgtype_encoded       | (10,[1],[1.0])       \n",
      " mqtt_conack_val_encoded    | (1,[0],[1.0])        \n",
      " features                   | (38,[0,25,28,37],... \n",
      " outcome                    | 2.0                  \n",
      "-RECORD 1------------------------------------------\n",
      " tcp_flags                  | 20.295459451058942   \n",
      " tcp_time_delta             | 1.0E-6               \n",
      " tcp_len                    | 51.0                 \n",
      " mqtt_conack_flags_reserved | 0.0                  \n",
      " mqtt_conack_flags_sp       | 0.0                  \n",
      " mqtt_conack_val            | 0.0                  \n",
      " mqtt_conflag_cleansess     | 0.0                  \n",
      " mqtt_conflag_passwd        | 0.0                  \n",
      " mqtt_conflag_qos           | 0.0                  \n",
      " mqtt_conflag_reserved      | 0.0                  \n",
      " mqtt_conflag_retain        | 0.0                  \n",
      " mqtt_conflag_uname         | 0.0                  \n",
      " mqtt_conflag_willflag      | 0.0                  \n",
      " mqtt_dupflag               | 1.0                  \n",
      " mqtt_kalive                | 0.0                  \n",
      " mqtt_len                   | 171.0                \n",
      " mqtt_msg                   | 4.630456339613062... \n",
      " mqtt_msgid                 | 5754.0               \n",
      " mqtt_msgtype               | 3.0                  \n",
      " mqtt_proto_len             | 0.0                  \n",
      " mqtt_qos                   | 1.0                  \n",
      " mqtt_retain                | 0.0                  \n",
      " mqtt_sub_qos               | 0.0                  \n",
      " mqtt_suback_qos            | 0.0                  \n",
      " mqtt_ver                   | 0.0                  \n",
      " mqtt_willmsg               | 0.0                  \n",
      " mqtt_willmsg_len           | 0.0                  \n",
      " mqtt_willtopic             | 0.0                  \n",
      " mqtt_willtopic_len         | 0.0                  \n",
      " mqtt_msgtype_encoded       | (10,[0],[1.0])       \n",
      " mqtt_conack_val_encoded    | (1,[0],[1.0])        \n",
      " features                   | (38,[0,1,12,13,14... \n",
      " outcome                    | 1.0                  \n",
      "-RECORD 2------------------------------------------\n",
      " tcp_flags                  | 20.295459451058942   \n",
      " tcp_time_delta             | 0.99982              \n",
      " tcp_len                    | 13.0                 \n",
      " mqtt_conack_flags_reserved | 0.0                  \n",
      " mqtt_conack_flags_sp       | 0.0                  \n",
      " mqtt_conack_val            | 0.0                  \n",
      " mqtt_conflag_cleansess     | 0.0                  \n",
      " mqtt_conflag_passwd        | 0.0                  \n",
      " mqtt_conflag_qos           | 0.0                  \n",
      " mqtt_conflag_reserved      | 0.0                  \n",
      " mqtt_conflag_retain        | 0.0                  \n",
      " mqtt_conflag_uname         | 0.0                  \n",
      " mqtt_conflag_willflag      | 0.0                  \n",
      " mqtt_dupflag               | 0.0                  \n",
      " mqtt_kalive                | 0.0                  \n",
      " mqtt_len                   | 11.0                 \n",
      " mqtt_msg                   | 31.0                 \n",
      " mqtt_msgid                 | 0.0                  \n",
      " mqtt_msgtype               | 3.0                  \n",
      " mqtt_proto_len             | 0.0                  \n",
      " mqtt_qos                   | 0.0                  \n",
      " mqtt_retain                | 0.0                  \n",
      " mqtt_sub_qos               | 0.0                  \n",
      " mqtt_suback_qos            | 0.0                  \n",
      " mqtt_ver                   | 0.0                  \n",
      " mqtt_willmsg               | 0.0                  \n",
      " mqtt_willmsg_len           | 0.0                  \n",
      " mqtt_willtopic             | 0.0                  \n",
      " mqtt_willtopic_len         | 0.0                  \n",
      " mqtt_msgtype_encoded       | (10,[0],[1.0])       \n",
      " mqtt_conack_val_encoded    | (1,[0],[1.0])        \n",
      " features                   | (38,[0,1,12,13,25... \n",
      " outcome                    | 0.0                  \n",
      "-RECORD 3------------------------------------------\n",
      " tcp_flags                  | 20.295459451058942   \n",
      " tcp_time_delta             | 0.007949             \n",
      " tcp_len                    | 0.0                  \n",
      " mqtt_conack_flags_reserved | 0.0                  \n",
      " mqtt_conack_flags_sp       | 0.0                  \n",
      " mqtt_conack_val            | 0.0                  \n",
      " mqtt_conflag_cleansess     | 0.0                  \n",
      " mqtt_conflag_passwd        | 0.0                  \n",
      " mqtt_conflag_qos           | 0.0                  \n",
      " mqtt_conflag_reserved      | 0.0                  \n",
      " mqtt_conflag_retain        | 0.0                  \n",
      " mqtt_conflag_uname         | 0.0                  \n",
      " mqtt_conflag_willflag      | 0.0                  \n",
      " mqtt_dupflag               | 0.0                  \n",
      " mqtt_kalive                | 0.0                  \n",
      " mqtt_len                   | 0.0                  \n",
      " mqtt_msg                   | 0.0                  \n",
      " mqtt_msgid                 | 0.0                  \n",
      " mqtt_msgtype               | 0.0                  \n",
      " mqtt_proto_len             | 0.0                  \n",
      " mqtt_qos                   | 0.0                  \n",
      " mqtt_retain                | 0.0                  \n",
      " mqtt_sub_qos               | 0.0                  \n",
      " mqtt_suback_qos            | 0.0                  \n",
      " mqtt_ver                   | 0.0                  \n",
      " mqtt_willmsg               | 0.0                  \n",
      " mqtt_willmsg_len           | 0.0                  \n",
      " mqtt_willtopic             | 0.0                  \n",
      " mqtt_willtopic_len         | 0.0                  \n",
      " mqtt_msgtype_encoded       | (10,[1],[1.0])       \n",
      " mqtt_conack_val_encoded    | (1,[0],[1.0])        \n",
      " features                   | (38,[0,25,28,37],... \n",
      " outcome                    | 2.0                  \n",
      "-RECORD 4------------------------------------------\n",
      " tcp_flags                  | 20.295459451058942   \n",
      " tcp_time_delta             | 1.21E-4              \n",
      " tcp_len                    | 0.0                  \n",
      " mqtt_conack_flags_reserved | 0.0                  \n",
      " mqtt_conack_flags_sp       | 0.0                  \n",
      " mqtt_conack_val            | 0.0                  \n",
      " mqtt_conflag_cleansess     | 0.0                  \n",
      " mqtt_conflag_passwd        | 0.0                  \n",
      " mqtt_conflag_qos           | 0.0                  \n",
      " mqtt_conflag_reserved      | 0.0                  \n",
      " mqtt_conflag_retain        | 0.0                  \n",
      " mqtt_conflag_uname         | 0.0                  \n",
      " mqtt_conflag_willflag      | 0.0                  \n",
      " mqtt_dupflag               | 0.0                  \n",
      " mqtt_kalive                | 0.0                  \n",
      " mqtt_len                   | 0.0                  \n",
      " mqtt_msg                   | 0.0                  \n",
      " mqtt_msgid                 | 0.0                  \n",
      " mqtt_msgtype               | 0.0                  \n",
      " mqtt_proto_len             | 0.0                  \n",
      " mqtt_qos                   | 0.0                  \n",
      " mqtt_retain                | 0.0                  \n",
      " mqtt_sub_qos               | 0.0                  \n",
      " mqtt_suback_qos            | 0.0                  \n",
      " mqtt_ver                   | 0.0                  \n",
      " mqtt_willmsg               | 0.0                  \n",
      " mqtt_willmsg_len           | 0.0                  \n",
      " mqtt_willtopic             | 0.0                  \n",
      " mqtt_willtopic_len         | 0.0                  \n",
      " mqtt_msgtype_encoded       | (10,[1],[1.0])       \n",
      " mqtt_conack_val_encoded    | (1,[0],[1.0])        \n",
      " features                   | (38,[0,25,28,37],... \n",
      " outcome                    | 1.0                  \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nslkdd_df_test.show(5, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81bccab5-3a90-49ad-a9da-4dc35202fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|outcome|count|\n",
      "+-------+-----+\n",
      "|    0.0|49639|\n",
      "|    1.0|39077|\n",
      "|    4.0| 4351|\n",
      "|    3.0| 2761|\n",
      "|    2.0| 3278|\n",
      "|    5.0|  184|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nslkdd_df_test.groupBy(\"outcome\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3fe3bfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o5403.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 80.0 failed 1 times, most recent failure: Lost task 0.0 in stage 80.0 (TID 60) (172.25.7.152 executor driver): java.lang.RuntimeException: Vector values MUST NOT be NaN or Infinity, but got (38,[0,1,12,13,14,16,25,26,27,37],[3.4473628112963306E-7,0.053824635906756645,2.7890577143082704,NaN,2.52808766770505,2.707404671193102,4.510688275293885,4.487015133212843,2.0022560887606677,15.170003569347903])\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1262)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1199)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1193)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1286)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1253)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1239)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1239)\r\n\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:517)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.lang.RuntimeException: Vector values MUST NOT be NaN or Infinity, but got (38,[0,1,12,13,14,16,25,26,27,37],[3.4473628112963306E-7,0.053824635906756645,2.7890577143082704,NaN,2.52808766770505,2.707404671193102,4.510688275293885,4.487015133212843,2.0022560887606677,15.170003569347903])\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1262)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prati\\Desktop\\College\\Fall 2023\\Systems(14763)\\Course Project\\course-project-option-2-satijapratik\\machineLearning.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prati/Desktop/College/Fall%202023/Systems%2814763%29/Course%20Project/course-project-option-2-satijapratik/machineLearning.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassification\u001b[39;00m \u001b[39mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prati/Desktop/College/Fall%202023/Systems%2814763%29/Course%20Project/course-project-option-2-satijapratik/machineLearning.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m lr \u001b[39m=\u001b[39m LogisticRegression(featuresCol \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m, labelCol \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39moutcome\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/prati/Desktop/College/Fall%202023/Systems%2814763%29/Course%20Project/course-project-option-2-satijapratik/machineLearning.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m lrModel \u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39;49mfit(nslkdd_df_test)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[0;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o5403.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 80.0 failed 1 times, most recent failure: Lost task 0.0 in stage 80.0 (TID 60) (172.25.7.152 executor driver): java.lang.RuntimeException: Vector values MUST NOT be NaN or Infinity, but got (38,[0,1,12,13,14,16,25,26,27,37],[3.4473628112963306E-7,0.053824635906756645,2.7890577143082704,NaN,2.52808766770505,2.707404671193102,4.510688275293885,4.487015133212843,2.0022560887606677,15.170003569347903])\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1262)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1199)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1193)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1286)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1253)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1239)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1239)\r\n\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:517)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.lang.RuntimeException: Vector values MUST NOT be NaN or Infinity, but got (38,[0,1,12,13,14,16,25,26,27,37],[3.4473628112963306E-7,0.053824635906756645,2.7890577143082704,NaN,2.52808766770505,2.707404671193102,4.510688275293885,4.487015133212843,2.0022560887606677,15.170003569347903])\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\r\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\r\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1262)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "#Model 1 - Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome')\n",
    "\n",
    "lrModel = lr.fit(nslkdd_df_test) # fit the logistic regression model to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o9172.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 126.0 failed 1 times, most recent failure: Lost task 0.0 in stage 126.0 (TID 94) (172.25.7.152 executor driver): java.lang.RuntimeException: Vector values MUST NOT be NaN or Infinity, but got (61,[0,1,12,13,26,36,37,44,45,49],[0.3443456949231132,0.01055385017779542,0.13048223224834013,NaN,2.0022560887606677,15.170003569347903,2.0128584380147863,10.273392652822842,10.271097817264629,2.156715641048752])\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1489)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1489)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1462)\r\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.lang.RuntimeException: Vector values MUST NOT be NaN or Infinity, but got (61,[0,1,12,13,26,36,37,44,45,49],[0.3443456949231132,0.01055385017779542,0.13048223224834013,NaN,2.0022560887606677,15.170003569347903,2.0128584380147863,10.273392652822842,10.271097817264629,2.156715641048752])\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1489)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prati\\Desktop\\College\\Fall 2023\\Systems(14763)\\Course Project\\course-project-option-2-satijapratik\\machineLearning.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prati/Desktop/College/Fall%202023/Systems%2814763%29/Course%20Project/course-project-option-2-satijapratik/machineLearning.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassification\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prati/Desktop/College/Fall%202023/Systems%2814763%29/Course%20Project/course-project-option-2-satijapratik/machineLearning.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m rf \u001b[39m=\u001b[39m RandomForestClassifier(featuresCol \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m, labelCol \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39moutcome\u001b[39m\u001b[39m'\u001b[39m,numTrees\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/prati/Desktop/College/Fall%202023/Systems%2814763%29/Course%20Project/course-project-option-2-satijapratik/machineLearning.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m rf_model \u001b[39m=\u001b[39m rf\u001b[39m.\u001b[39;49mfit(nslkdd_df)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[0;32m    382\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o9172.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 126.0 failed 1 times, most recent failure: Lost task 0.0 in stage 126.0 (TID 94) (172.25.7.152 executor driver): java.lang.RuntimeException: Vector values MUST NOT be NaN or Infinity, but got (61,[0,1,12,13,26,36,37,44,45,49],[0.3443456949231132,0.01055385017779542,0.13048223224834013,NaN,2.0022560887606677,15.170003569347903,2.0128584380147863,10.273392652822842,10.271097817264629,2.156715641048752])\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1489)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1489)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1462)\r\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.lang.RuntimeException: Vector values MUST NOT be NaN or Infinity, but got (61,[0,1,12,13,26,36,37,44,45,49],[0.3443456949231132,0.01055385017779542,0.13048223224834013,NaN,2.0022560887606677,15.170003569347903,2.0128584380147863,10.273392652822842,10.271097817264629,2.156715641048752])\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1489)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'outcome',numTrees=20)\n",
    "rf_model = rf.fit(nslkdd_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbeb67b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
