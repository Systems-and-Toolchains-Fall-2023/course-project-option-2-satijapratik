{"cells":[{"cell_type":"code","execution_count":1,"id":"3cd682ae-d094-4bae-84fe-2e3f10aa9893","metadata":{},"outputs":[],"source":["# import findspark\n","# findspark.init()\n","# findspark.find()\n","import pyspark\n","from pyspark.sql import SparkSession, SQLContext\n","from pyspark.ml import Pipeline,Transformer\n","from pyspark.ml.feature import Imputer,StandardScaler,StringIndexer,OneHotEncoder, VectorAssembler\n","\n","from pyspark.sql.functions import col, SparkContext, udf\n","from pyspark.sql.types import *\n","import numpy as np"]},{"cell_type":"code","execution_count":3,"id":"589cccdf-5556-40ea-a306-484322f56f29","metadata":{},"outputs":[],"source":["appName = \"Big Data ML\"\n","master = \"local\"\n","\n","# Create Configuration object for Spark.\n","conf = pyspark.SparkConf()\\\n","    .set('spark.driver.host','127.0.0.1')\\\n","    .setAppName(appName)\\\n","    .setMaster(\"local[*]\")\n","\n","# Create Spark Context with the new configurations rather than relying on the default one\n","sc = SparkContext.getOrCreate(conf=conf)\n","\n","# You need to create SQL Context to conduct some database operations like what we will see later.\n","sqlContext = SQLContext(sc)\n","\n","# If you have SQL context, you create the session from the Spark Context\n","# Differnt config to point the config in the right direction for the location of the JARS file on the cloud\n","spark = SparkSession.builder \\\n","    .master(\"local[*]\") \\\n","    .appName(\"GenericAppName\") \\\n","    .config('sparks.jars.packages', \"org.postgresql:postgresql:42.6.0\") \\\n","    .getOrCreate() "]},{"cell_type":"code","execution_count":5,"id":"b5f531d3-81fe-413d-b5ba-cae974854464","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["\n","mqtt_raw = spark.read.csv(\"gs://dataproc-staging-us-central1-252073455644-vejdhxap/notebooks/jupyter/train70_reduced.csv\",header=True, inferSchema= True)\n","mqtt_test_raw = spark.read.csv(\"gs://dataproc-staging-us-central1-252073455644-vejdhxap/notebooks/jupyter/test30_reduced.csv\",header=True, inferSchema= True)\n","\n","# Rename columns\n","for column_name in mqtt_raw.columns:\n","    new_column_name = column_name.replace(\".\", \"_\")\n","    mqtt_raw = mqtt_raw.withColumnRenamed(column_name, new_column_name)\n","\n","for column_name in mqtt_test_raw.columns:\n","    new_column_name = column_name.replace(\".\", \"_\")\n","    mqtt_test_raw = mqtt_test_raw.withColumnRenamed(column_name, new_column_name)\n","\n","# Drop the few rows with NA values (only 190)\n","mqtt_raw = mqtt_raw.na.drop()\n","mqtt_test_raw = mqtt_test_raw.na.drop()\n","\n","# Dropping a couple useless columns\n","mqtt_raw = mqtt_raw.drop(\"mqtt_protoname\")\n","mqtt_test_raw = mqtt_test_raw.drop(\"mqtt_protoname\")\n","\n","# Converting nominal columns to string so they can be encoded by the transformer \n","# mqtt_raw = mqtt_raw.withColumn(\"mqtt_msgtype\", mqtt_raw[\"mqtt_msgtype\"].cast(\"string\"))\n","# mqtt_raw = mqtt_raw.withColumn(\"mqtt_conack_val\", mqtt_raw[\"mqtt_conack_val\"].cast(\"string\"))\n","# mqtt_test_raw = mqtt_test_raw.withColumn(\"mqtt_msgtype\", mqtt_test_raw[\"mqtt_msgtype\"].cast(\"string\"))\n","# mqtt_test_raw = mqtt_test_raw.withColumn(\"mqtt_conack_val\", mqtt_test_raw[\"mqtt_conack_val\"].cast(\"string\"))"]},{"cell_type":"code","execution_count":null,"id":"b9e8735c-24c2-480a-b3fc-206551fe1504","metadata":{},"outputs":[],"source":["mqtt_raw.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"e4be9718-6e09-46cd-a3e9-05a6b3ede717","metadata":{},"outputs":[],"source":["col_names =  [\"tcp_flags\",\"tcp_time_delta\",\"tcp_len\",\n","\"mqtt_conack_flags\",\"mqtt_conack_flags_reserved\",\"mqtt_conack_flags_sp\",\n","\"mqtt_conack_val\",\"mqtt_conflag_cleansess\",\"mqtt_conflag_passwd\",\n","\"mqtt_conflag_qos\",\"mqtt_conflag_reserved\",\"mqtt_conflag_retain\",\n","\"mqtt_conflag_uname\",\"mqtt_conflag_willflag\",\n","\"mqtt_conflags\",\"mqtt_dupflag\",\"mqtt_hdrflags\",\n","\"mqtt_kalive\",\"mqtt_len\",\"mqtt_msg\",\"mqtt_msgid\",\"mqtt_msgtype\",\n","\"mqtt_proto_len\",\"mqtt_qos\",\n","\"mqtt_retain\",\"mqtt_sub_qos\",\"mqtt_suback_qos\",\"mqtt_ver\",\n","\"mqtt_willmsg\",\"mqtt_willmsg_len\",\"mqtt_willtopic\",\"mqtt_willtopic_len\",\n","\"target\"]\n","\n","nominal_cols = [\"mqtt_msgtype\", \"mqtt_conack_val\", \"tcp_flags\", \"mqtt_hdrflags\"]\n","binary_cols = [\"mqtt_dupflag\"]\n","continuous_cols = [\"tcp_time_delta\",\"tcp_len\",\n","\"mqtt_conack_flags_reserved\",\"mqtt_conack_flags_sp\",\n","\"mqtt_conflag_cleansess\",\"mqtt_conflag_passwd\",\n","\"mqtt_conflag_qos\",\"mqtt_conflag_reserved\",\"mqtt_conflag_retain\",\n","\"mqtt_conflag_uname\",\"mqtt_conflag_willflag\",\n","\"mqtt_kalive\",\"mqtt_len\",\"mqtt_msg\",\"mqtt_msgid\",\n","\"mqtt_proto_len\",\"mqtt_qos\",\n","\"mqtt_retain\",\"mqtt_sub_qos\",\"mqtt_suback_qos\",\"mqtt_ver\",\n","\"mqtt_willmsg\",\"mqtt_willmsg_len\",\"mqtt_willtopic\",\"mqtt_willtopic_len\",\n","\"mqtt_conack_flags\", \"mqtt_conflags\"]"]},{"cell_type":"code","execution_count":null,"id":"69ee95a4-ceff-4ec8-9a9d-d8b84cedf9c6","metadata":{},"outputs":[],"source":["# Add any columns that we want to just get rid of here\n","# A large portion of these were dropped because they were a constant value\n","columns_to_drop = [\"mqtt_msg\", \"mqtt_conack_flags\", \"mqtt_conflags\", \n","\"mqtt_willtopic_len\", \"mqtt_willtopic\", \"mqtt_willmsg_len\", \"mqtt_willmsg\",\n","\"mqtt_suback_qos\", \"mqtt_sub_qos\", \"mqtt_conflag_willflag\", \"mqtt_conflag_retain\",\n","\"mqtt_conflag_reserved\", \"mqtt_conflag_qos\", \"mqtt_conack_flags_sp\", \n","\"mqtt_conack_flags_reserved\"] "]},{"cell_type":"code","execution_count":null,"id":"48d960f5-59bd-4901-a8d5-a02777941b9e","metadata":{},"outputs":[],"source":["'''\n","Data preprocessing pipeline\n","'''\n","class OutcomeCreater(Transformer): # this defines a transformer that creates the outcome column\n","    def __init__(self):\n","        super().__init__()\n","\n","    def label_to_vector(self, name):\n","        name = name.lower()\n","        \n","        if name == 'legitimate':\n","            return 0.0\n","        elif name == 'dos':\n","            return 1.0\n","        elif name == 'malformed':\n","            return 2.0\n","        elif name == 'slowite':\n","            return 3.0\n","        elif name == 'bruteforce':\n","            return 4.0\n","        elif name == 'flood':\n","            return 5.0\n","        else:\n","            return -100.0\n","\n","    def _transform(self, dataset):\n","        label_to_vector_udf = udf(self.label_to_vector, StringType())\n","        output_df = dataset.withColumn('outcome', label_to_vector_udf(col('target'))).drop(\"target\")  \n","        output_df = output_df.withColumn('outcome', col('outcome').cast(DoubleType()))\n","        return output_df\n","\n","class FeatureTypeCaster(Transformer): # this transformer will cast the columns as appropriate types  \n","    def __init__(self, targetCols):\n","        super().__init__()\n","        self.target_cols = targetCols\n","\n","    def _transform(self, dataset):\n","        output_df = dataset\n","        for col_name in self.target_cols:\n","            output_df = output_df.withColumn(col_name,col(col_name).cast(DoubleType()))\n","\n","        return output_df\n","\n","class ColumnDropper(Transformer): # this transformer drops unnecessary columns\n","    def __init__(self, columns_to_drop = None):\n","        super().__init__()\n","        self.columns_to_drop=columns_to_drop\n","    def _transform(self, dataset):\n","        output_df = dataset\n","        for col_name in self.columns_to_drop:\n","            output_df = output_df.drop(col_name)\n","        return output_df\n","\n","def get_preprocess_pipeline():\n","    # Stage where columns are casted as appropriate types\n","    impute_columns = binary_cols+continuous_cols\n","    for col_name in columns_to_drop:\n","        if col_name in impute_columns:\n","            impute_columns.remove(col_name)\n","    stage_typecaster = FeatureTypeCaster(impute_columns)\n","\n","    # Stage where coulmns are imputed if they have NAs\n","    stage_imputer = Imputer(inputCols=impute_columns, outputCols=impute_columns)\n","\n","    # Stage where nominal columns are transformed to index columns using StringIndexer\n","    nominal_id_cols = [x+\"_index\" for x in nominal_cols]\n","    nominal_onehot_cols = [x+\"_encoded\" for x in nominal_cols]\n","    stage_nominal_indexer = StringIndexer(inputCols=nominal_cols, outputCols=nominal_id_cols)\n","\n","    # Stage where the index columns are further transformed using OneHotEncoder\n","    stage_nominal_onehot_encoder = OneHotEncoder(inputCols=nominal_id_cols, outputCols=nominal_onehot_cols)\n","\n","    # Stage where all relevant features are assembled into a vector (and dropping a few)\n","    feature_cols = impute_columns+nominal_onehot_cols\n","    for col_name in columns_to_drop:\n","        if col_name in nominal_cols:\n","            feature_cols.remove(col_name+\"_encoded\")\n","            feature_cols.remove(col_name+\"_index\")\n","    stage_vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"vectorized_features\")\n","\n","    # Stage where we scale the columns\n","    stage_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n","\n","    # Stage for creating the outcome column representing whether there is attack \n","    stage_outcome = OutcomeCreater()\n","\n","    # Removing all unnecessary columbs, only keeping the 'features' and 'outcome' columns\n","    stage_column_dropper = ColumnDropper(columns_to_drop = nominal_cols + nominal_id_cols + nominal_onehot_cols +\n","        binary_cols + continuous_cols + columns_to_drop + ['vectorized_features'])\n","    # stage_column_dropper = ColumnDropper(columns_to_drop = nominal_id_cols + columns_to_drop + ['vectorized_features'])\n","    # Connect the columns into a pipeline\n","    pipeline = Pipeline(stages=[stage_typecaster, stage_imputer, stage_nominal_indexer, stage_nominal_onehot_encoder,\n","        stage_vector_assembler, stage_scaler, stage_outcome, stage_column_dropper])\n","    return pipeline"]},{"cell_type":"code","execution_count":null,"id":"baad9e78-144a-44bb-80a0-163113755380","metadata":{},"outputs":[],"source":["preprocess_pipeline = get_preprocess_pipeline()\n","preprocess_pipeline_model = preprocess_pipeline.fit(mqtt_raw)\n","\n","mqtt_df = preprocess_pipeline_model.transform(mqtt_raw)\n","mqtt_df_test = preprocess_pipeline_model.transform(mqtt_test_raw)\n","\n","mqtt_df.cache()\n","mqtt_df_test.cache()"]},{"cell_type":"code","execution_count":null,"id":"16270dcf-1b3e-4d93-adc7-ded87b584be8","metadata":{},"outputs":[],"source":["mqtt_df_test.printSchema()"]},{"cell_type":"code","execution_count":null,"id":"7e83acfd-cb1d-44c2-b77c-36d16be414e0","metadata":{},"outputs":[],"source":["mqtt_df_test.show(3, truncate=False, vertical=True)"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Pyspark ML Models</h1>"]},{"cell_type":"code","execution_count":null,"id":"c3fe3bfe","metadata":{},"outputs":[],"source":["#Pyspark ml model 1 - Logistic Regression\n","from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n","\n","lr = LogisticRegression(featuresCol = 'features', labelCol = 'outcome')\n","lrModel = lr.fit(mqtt_df) "]},{"cell_type":"code","execution_count":null,"id":"f184a264","metadata":{},"outputs":[],"source":["#Pyspark ml model 2 - Random Forest Decision Tree\n","from pyspark.ml.classification import RandomForestClassifier\n","\n","rfc = RandomForestClassifier(featuresCol = 'features', labelCol = 'outcome')\n","rf_model = rfc.fit(mqtt_df)"]},{"cell_type":"code","execution_count":null,"id":"da30e7bb-acc8-4780-a83e-8f1566384f97","metadata":{},"outputs":[],"source":["pred_LR = lrModel.transform(mqtt_df_test)\n","y_pred = pred_LR.select('prediction').collect()\n","y = mqtt_df_test.select('outcome').collect()\n","correct = 0\n","for i in range(len(y)):\n","    if y[i][0]==y_pred[i][0]:\n","        correct += 1\n","print(\"Logistic Regression Test Accuracy:\",correct/len(y)*100, \"%\")"]},{"cell_type":"code","execution_count":null,"id":"fbb57f7a-e526-42fa-873c-143ed9a4f504","metadata":{},"outputs":[],"source":["pred_DT = rf_model.transform(mqtt_df_test)\n","y_pred_DT = pred_DT.select('prediction').collect()\n","correct = 0\n","for i in range(len(y)):\n","    if y[i][0]==y_pred_DT[i][0]:\n","        correct += 1\n","print(\"Decision Tree Test Accuracy:\",correct/len(y)*100, \"%\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","# Hyperparameters for Linear Regression\n","lr_paramGrid = (ParamGridBuilder()\n","#                 .addGrid(lr.regParam, [0.005, 0.01, 0.05])\n","                .addGrid(lr.regParam, [0.05])\n","                .addGrid(lr.maxIter, [5, 10])\n","                .build())\n","\n","evaluator = MulticlassClassificationEvaluator(predictionCol='prediction',\n","                                        labelCol='outcome',\n","                                        metricName='accuracy')\n","\n","lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=lr_paramGrid,\n","                       evaluator=evaluator, numFolds=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lr_cv_model = lr_cv.fit(mqtt_df)\n","\n","lr_cv_prediction_test = lr_cv_model.transform(mqtt_df_test)\n","print('Linear Regression Hyperparameter tuning results: ')\n","# print('Test Area under ROC (AUC) before Cross-Validation:', evaluator.evaluate(lr_test_predictions))\n","print('Test Area under ROC (AUC) after Cross-Validation:', evaluator.evaluate(lr_cv_prediction_test))\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Hyperparameters for Random Forest\n","rf_paramGrid = (ParamGridBuilder()\n","                .addGrid(rfc.maxDepth, [5, 10])\n","                # .addGrid(rfc.numTrees, [10, 20, 40])\n","                .build())\n","\n","evaluator = MulticlassClassificationEvaluator(predictionCol='prediction',\n","                                        labelCol='outcome',\n","                                        metricName='accuracy')\n","\n","rf_cv = CrossValidator(estimator=rfc, estimatorParamMaps=rf_paramGrid,\n","                       evaluator=evaluator, numFolds=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rf_cv_model = rf_cv.fit(mqtt_df)\n","\n","rf_cv_prediction_test = rf_cv_model.transform(mqtt_df_test)\n","print('Random Forest Hyperparameter tuning results: ')\n","# print('Test Area under ROC (AUC) before Cross-Validation:', evaluator.evaluate(rfc_test_predictions))\n","print('Test Area under ROC (AUC) after Cross-Validation:', evaluator.evaluate(rf_cv_prediction_test))\n","print()"]},{"cell_type":"markdown","id":"9af7fbed-a0b9-40ef-af16-d5300a4ae96b","metadata":{},"source":["<h1>Pytorch Learning</h1>"]},{"cell_type":"code","execution_count":null,"id":"86a6eb6c-147c-43cb-ac22-21970c7d9f42","metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch import nn, optim\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"id":"75c4f827-b10c-4d6e-a40b-21617b470df4","metadata":{},"outputs":[],"source":["mqtt_df_val, mqtt_df_test = mqtt_df_test.randomSplit([0.5, 0.5])"]},{"cell_type":"code","execution_count":null,"id":"7c020bfa-ac00-4109-9e3f-470cc2aee771","metadata":{},"outputs":[],"source":["mqtt_df_val_pd = mqtt_df_val.toPandas()\n","mqtt_df_train_pd = mqtt_df.toPandas()\n","mqtt_df_test_pd = mqtt_df_test.toPandas()"]},{"cell_type":"code","execution_count":null,"id":"56e49686-e204-4a96-85c2-2c20cb62e191","metadata":{},"outputs":[],"source":["x_train = torch.from_numpy(np.array(mqtt_df_train_pd['features'].values.tolist(),np.float32))\n","y_train = torch.from_numpy(np.array(mqtt_df_train_pd['outcome'].values.tolist(),np.int64))\n","\n","x_test = torch.from_numpy(np.array(mqtt_df_test_pd['features'].values.tolist(),np.float32))\n","y_test = torch.from_numpy(np.array(mqtt_df_test_pd['outcome'].values.tolist(),np.int64))\n","\n","x_val = torch.from_numpy(np.array(mqtt_df_val_pd['features'].values.tolist(),np.float32))\n","y_val = torch.from_numpy(np.array(mqtt_df_val_pd['outcome'].values.tolist(),np.int64))"]},{"cell_type":"code","execution_count":null,"id":"457e701a-cd7c-4f50-b24b-4000e8094a97","metadata":{},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self,x,y):\n","        self.x = x\n","        self.y = y\n","\n","    def __len__(self):\n","        return self.x.shape[0]\n","\n","    def __getitem__(self,idx):\n","        return(self.x[idx],self.y[idx])\n","\n","train_dataset = MyDataset(x_train,y_train)\n","validate_dataset = MyDataset(x_val,y_val)\n","test_dataset = MyDataset(x_test,y_test)"]},{"cell_type":"code","execution_count":null,"id":"c59c1095-7a46-4965-a1cf-2e5f314d721b","metadata":{},"outputs":[],"source":["class NeuralNet(nn.Module):\n","    def __init__(self, input_size=2, output_size=1, hidden_layer_sizes=[24,24], activation_func = nn.ReLU):\n","        super().__init__()\n","\n","        layers = []                                               # List of layers\n","        layers.append(nn.Linear(input_size,hidden_layer_sizes[0]))         # First hidden layer\n","\n","                                                                  # Next hidden layers\n","        for i in range(len(hidden_layer_sizes)-1):\n","            layers.append(activation_func())\n","            layers.append(nn.Linear(hidden_layer_sizes[i],hidden_layer_sizes[i+1]))\n","            \n","        layers.append(activation_func())                                  # Output layer (if needed)\n","        layers.append(nn.Linear(hidden_layer_sizes[-1], output_size))\n","        \n","        self.seq = nn.Sequential(*layers)                         # Final sequential model\n","\n","    def forward(self, xy):\n","        return self.seq(xy)"]},{"cell_type":"code","execution_count":null,"id":"54136f93-1515-4c5e-8dc0-317464092244","metadata":{},"outputs":[],"source":["def train(model, train_data, val_data, lr = 0.01, epochs = 1000, batch_size=32, gamma = 1):\n","    # Define loss, optimizer, and scheduler\n","    lossfun = nn.functional.cross_entropy\n","    opt = optim.SGD(model.parameters(), lr=lr)\n","    scheduler = optim.lr_scheduler.ExponentialLR(optimizer=opt, gamma=gamma)\n","\n","    # Define data loaders to do batch decent\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n","\n","    train_loss_hist = []\n","    val_loss_hist = []\n","    train_acc_hist = []\n","    val_acc_hist = []\n","    batch_loss_hist = []\n","    best_acc = 0.0\n","    for epoch in range(epochs+1):\n","\n","        # Training batch loop for this epoch\n","        train_batch_loss=[]\n","        train_batch_acc=[]\n","        model.train()\n","        for _, (x_batch, y_batch) in enumerate(train_dataloader):\n","            test_batch_pred = model(x_batch)\n","            loss_train = lossfun(test_batch_pred, y_batch)\n","            \n","            opt.zero_grad()\n","            loss_train.backward()\n","            opt.step()\n","\n","            train_batch_loss.append(loss_train.detach())\n","            test_batch_label = np.argmax(test_batch_pred.detach(), axis=1).numpy()\n","            train_batch_acc.append(np.sum(test_batch_label == y_batch.numpy())/y_batch.shape[0])\n","\n","        # Validation batch loop for this epoch\n","        val_batch_loss = []\n","        val_batch_acc=[]\n","        model.eval()\n","        for _, (x_batch_val, y_batch_val) in enumerate(val_dataloader):\n","            val_batch_pred = model(x_batch_val)\n","            loss_val = lossfun(val_batch_pred, y_batch_val)\n","\n","            val_batch_loss.append(loss_val.detach())\n","            val_batch_label = np.argmax(val_batch_pred.detach(), axis=1).numpy()\n","            val_batch_acc.append(np.sum(val_batch_label == y_batch_val.numpy())/y_batch_val.shape[0])\n","\n","        scheduler.step()\n","\n","        # Save data to histories\n","        batch_loss_hist += train_batch_loss\n","        train_loss_hist.append(np.mean(train_batch_loss))\n","        val_loss_hist.append(np.mean(val_batch_loss))\n","        train_acc_hist.append(np.mean(train_batch_acc))\n","        val_acc_hist.append(np.mean(val_batch_acc))\n","\n","        # Print epoch info\n","        # print(f\"Epoch {epoch:>4} of {epochs}:   Train Loss = {   train_loss_hist[-1]:.3f}   Validation Loss = {   val_loss_hist[-1]:.3f}\"+\n","        #                                    f\"   Train Acc  = {100*train_acc_hist[-1]:.3f}   Validation Acc  = {100*val_acc_hist[-1]:.3f}\")\n","\n","        # Save model if it is the best so far\n","        if val_acc_hist[-1] > best_acc:\n","            torch.save(model.state_dict(), 'current_best_model')\n","            best_acc = val_acc_hist[-1]\n","\n","    plt.figure(figsize=(4,2),dpi=250)\n","    plt.plot(batch_loss_hist)\n","    plt.xlabel(\"Batch\")\n","    plt.ylabel(\"Loss\")\n","    plt.title(\"Loss across all batches\")\n","    plt.show()\n","\n","    fig, axs = plt.subplots(1,2)\n","    axs[0].plot(train_loss_hist,label=\"Training\")\n","    axs[0].plot(val_loss_hist,label=\"Validation\")\n","    axs[0].legend()\n","    axs[0].set_xlabel(\"Epoch\")\n","    axs[0].set_ylabel(\"Loss\")\n","    axs[0].set_title(\"Loss across Epochs\")\n","\n","    axs[1].plot(train_acc_hist,label=\"Training\")\n","    axs[1].plot(val_acc_hist,label=\"Validation\")\n","    axs[1].legend()\n","    axs[1].set_xlabel(\"Epoch\")\n","    axs[1].set_ylabel(\"Accuracy\")\n","    axs[1].set_title(\"Accuracy across Epochs\")\n","    plt.show()\n","\n","    return"]},{"cell_type":"code","execution_count":null,"id":"773c70a3-21c2-43a5-b040-999f529e8e87","metadata":{},"outputs":[],"source":["# Pytorch model #1 - Deep Nueral Network\n","hidden_layers = [128,128,128,128]\n","activation = nn.RReLU\n","net = NeuralNet(input_size=x_train.shape[1], output_size=6, hidden_layer_sizes=hidden_layers, activation_func=activation)\n","print(net)\n","train(net, train_dataset, validate_dataset, lr=0.005, epochs=20, batch_size=64, gamma=0.995)"]},{"cell_type":"code","execution_count":null,"id":"9b14a1e8-6a16-4da3-b0fd-4fee35235ab5","metadata":{},"outputs":[],"source":["best_model = NeuralNet(input_size=x_train.shape[1], output_size=6, hidden_layer_sizes=hidden_layers, activation_func=activation)\n","best_model.load_state_dict(torch.load(\"current_best_model\"))\n","\n","prediction = np.argmax(best_model(x_test).detach(), axis=1).numpy()\n","accuracy = 100*np.sum(prediction == y_test.numpy())/y_test.shape[0]\n","print(f\"Testing accuracy: {accuracy:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Pytorch model #2 - Shallow Nueral Network\n","hidden_layers = [8,8]\n","activation = nn.ReLU\n","net_shallow = NeuralNet(input_size=x_train.shape[1], output_size=6, hidden_layer_sizes=hidden_layers, activation_func=activation)\n","print(net_shallow)\n","train(net_shallow, train_dataset, validate_dataset, lr=0.05, epochs=10, batch_size=64, gamma=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_model_shallow = NeuralNet(input_size=x_train.shape[1], output_size=6, hidden_layer_sizes=hidden_layers, activation_func=activation)\n","best_model_shallow.load_state_dict(torch.load(\"current_best_model\"))\n","\n","prediction_shallow = np.argmax(best_model_shallow(x_test).detach(), axis=1).numpy()\n","accuracy_shallow = 100*np.sum(prediction_shallow == y_test.numpy())/y_test.shape[0]\n","print(f\"Shallow network testing accuracy: {accuracy_shallow:.2f}%\")"]},{"cell_type":"markdown","id":"509c5b92","metadata":{},"source":["<h4>Task IV: </h4> Deploy code on the cloud"]},{"cell_type":"code","execution_count":6,"id":"db285573-de10-4d9b-8b08-fb5da496a91c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/11/15 22:39:23 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","                                                                                \r"]}],"source":["db_properties={}\n","#update your db username\n","db_properties['username']=\"postgres\"\n","#update your db password\n","db_properties['password']=\"pratik120\"\n","#make sure you got the right port number here\n","db_properties['url']= \"jdbc:postgresql://34.72.76.14:5432/postgres\"\n","#make sure you had the Postgres JAR file in the right location\n","db_properties['table']= \"MQTT_Test\"\n","\n","\n","\n","mqtt_test_raw.write.format(\"jdbc\")\\\n",".mode(\"overwrite\")\\\n",".option(\"url\", db_properties['url'])\\\n",".option(\"dbtable\", db_properties['table'])\\\n",".option(\"user\", db_properties['username'])\\\n",".option(\"password\", db_properties['password'])\\\n",".save()\n"]},{"cell_type":"code","execution_count":8,"id":"81192977","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 5:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["-RECORD 0--------------------------------\n"," tcp_flags                  | 0x00000014 \n"," tcp_time_delta             | 0.029854   \n"," tcp_len                    | 0          \n"," mqtt_conack_flags          | 0          \n"," mqtt_conack_flags_reserved | 0.0        \n"," mqtt_conack_flags_sp       | 0.0        \n"," mqtt_conack_val            | 0.0        \n"," mqtt_conflag_cleansess     | 0.0        \n"," mqtt_conflag_passwd        | 0.0        \n"," mqtt_conflag_qos           | 0.0        \n"," mqtt_conflag_reserved      | 0.0        \n"," mqtt_conflag_retain        | 0.0        \n"," mqtt_conflag_uname         | 0.0        \n"," mqtt_conflag_willflag      | 0.0        \n"," mqtt_conflags              | 0          \n"," mqtt_dupflag               | 0.0        \n"," mqtt_hdrflags              | 0          \n"," mqtt_kalive                | 0.0        \n"," mqtt_len                   | 0.0        \n"," mqtt_msg                   | 0          \n"," mqtt_msgid                 | 0.0        \n"," mqtt_msgtype               | 0.0        \n"," mqtt_proto_len             | 0.0        \n"," mqtt_qos                   | 0.0        \n"," mqtt_retain                | 0.0        \n"," mqtt_sub_qos               | 0.0        \n"," mqtt_suback_qos            | 0.0        \n"," mqtt_ver                   | 0.0        \n"," mqtt_willmsg               | 0.0        \n"," mqtt_willmsg_len           | 0.0        \n"," mqtt_willtopic             | 0.0        \n"," mqtt_willtopic_len         | 0.0        \n"," target                     | malformed  \n","only showing top 1 row\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df_read = sqlContext.read.format(\"jdbc\")\\\n","    .option(\"url\", db_properties['url'])\\\n","    .option(\"dbtable\", db_properties['table'])\\\n","    .option(\"user\", db_properties['username'])\\\n","    .option(\"password\", db_properties['password'])\\\n","    .load()\n","\n","df_read.show(1, vertical=True)"]},{"cell_type":"code","execution_count":null,"id":"4b65ffa5","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}
